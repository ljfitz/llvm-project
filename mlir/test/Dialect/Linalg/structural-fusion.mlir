// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// The script is designed to make adding checks to
// a test case fast, it is *not* designed to be authoritative
// about what constitutes a good test! The CHECK should be
// minimized and named to reflect the test intent.

// RUN: mlir-opt %s -split-input-file -linalg-unfuse -linalg-structural-fusion \
// RUN: | FileCheck %s

// CHECK-LABEL:   func @forward(
// CHECK-SAME:                  %[[VAL_0:.*]]: tensor<1x3x416x416xf32>) -> tensor<1x16x208x208xf32> {
// CHECK:           %[[VAL_1:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:           %[[VAL_2:.*]] = linalg.fused(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x3x416x416xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : f32) {
// CHECK:             %[[VAL_5:.*]] = tensor.pad %[[VAL_3]] low[0, 0, 1, 1] high[0, 0, 1, 1] {
// CHECK:             ^bb0(%[[VAL_6:.*]]: index, %[[VAL_7:.*]]: index, %[[VAL_8:.*]]: index, %[[VAL_9:.*]]: index):
// CHECK:               tensor.yield %[[VAL_4]] : f32
// CHECK:             } : tensor<1x3x416x416xf32> to tensor<1x3x418x418xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.init_tensor [1, 16, 416, 416] : tensor<1x16x416x416xf32>
// CHECK:             %[[VAL_11:.*]] = arith.constant dense<1.000000e+00> : tensor<16xf32>
// CHECK:             %[[VAL_12:.*]] = arith.constant dense<1.000000e+00> : tensor<16x3x3x3xf32>
// CHECK:             %[[VAL_13:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_14:.*]] = linalg.init_tensor [1, 16, 208, 208] : tensor<1x16x208x208xf32>
// CHECK:             %[[VAL_15:.*]] = linalg.init_tensor [2, 2] : tensor<2x2xf32>
// CHECK:             %[[VAL_16:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_11]] : tensor<16xf32>) outs(%[[VAL_10]] : tensor<1x16x416x416xf32>) -> tensor<1x16x416x416xf32>
// CHECK:             %[[VAL_17:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_18:.*]], %[[VAL_12]] : tensor<1x3x418x418xf32>, tensor<16x3x3x3xf32>) outs(%[[VAL_16]] : tensor<1x16x416x416xf32>) -> tensor<1x16x416x416xf32>
// CHECK:             %[[VAL_19:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_17]], %[[VAL_13]] : tensor<1x16x416x416xf32>, f32) outs(%[[VAL_17]] : tensor<1x16x416x416xf32>) -> tensor<1x16x416x416xf32>
// CHECK:             %[[VAL_20:.*]] = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_19]], %[[VAL_15]] : tensor<1x16x416x416xf32>, tensor<2x2xf32>) outs(%[[VAL_14]] : tensor<1x16x208x208xf32>) -> tensor<1x16x208x208xf32>
// CHECK:             linalg.yield %[[VAL_20]] : tensor<1x16x208x208xf32>
// CHECK:           } -> tensor<1x16x208x208xf32>
// CHECK:           return %[[VAL_21:.*]] : tensor<1x16x208x208xf32>
// CHECK:         }
func.func @forward(%arg0: tensor<1x3x416x416xf32>) -> tensor<1x16x208x208xf32> {
  %cst_0 = arith.constant dense<1.000000e+00> : tensor<16xf32>
  %cst_1 = arith.constant 0.000000e+00 : f32
  %cst_2 = arith.constant dense<1.000000e+00> : tensor<16x3x3x3xf32>
  %0 = linalg.init_tensor [2, 2] : tensor<2x2xf32>
  %1 = linalg.init_tensor [1, 16, 416, 416] : tensor<1x16x416x416xf32>
  %42 = linalg.init_tensor [1, 16, 208, 208] : tensor<1x16x208x208xf32>
  %2 = linalg.broadcast_bias_2d_fchw ins(%cst_0 : tensor<16xf32>) outs(%1 : tensor<1x16x416x416xf32>) -> tensor<1x16x416x416xf32>
  %3 = tensor.pad %arg0 low[0, 0, 1, 1] high[0, 0, 1, 1] {
      ^bb0(%arg8: index, %arg9: index, %arg10: index, %arg11: index):
      tensor.yield %cst_1 : f32
  } : tensor<1x3x416x416xf32> to tensor<1x3x418x418xf32>
  %4 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%3, %cst_2 : tensor<1x3x418x418xf32>, tensor<16x3x3x3xf32>) outs(%2 : tensor<1x16x416x416xf32>) -> tensor<1x16x416x416xf32>
  %5 = linalg.lrelu_2d_nchw ins(%4, %cst_1 : tensor<1x16x416x416xf32>, f32) outs(%4 : tensor<1x16x416x416xf32>) -> tensor<1x16x416x416xf32>
  %6 = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%5, %0 : tensor<1x16x416x416xf32>, tensor<2x2xf32>) outs(%42 : tensor<1x16x208x208xf32>) -> tensor<1x16x208x208xf32>
  return %6 : tensor<1x16x208x208xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_tensor_add(
// CHECK-SAME:                                    %[[VAL_0:.*]]: tensor<1x1024x10x10xf32>,
// CHECK-SAME:                                    %[[VAL_1:.*]]: tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32> {
// CHECK:           %[[VAL_2:.*]] = linalg.fused(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x1024x10x10xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : tensor<1x1024x8x8xf32>) {
// CHECK:             %[[VAL_5:.*]] = linalg.init_tensor [1, 1024, 8, 8] : tensor<1x1024x8x8xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_8:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_6]] : tensor<1024xf32>) outs(%[[VAL_5]] : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_3]], %[[VAL_7]] : tensor<1x1024x10x10xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_8]] : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32>
// CHECK:             %[[VAL_10:.*]] = arith.addf %[[VAL_9]], %[[VAL_4]] : tensor<1x1024x8x8xf32>
// CHECK:             linalg.yield %[[VAL_10]] : tensor<1x1024x8x8xf32>
// CHECK:           } -> tensor<1x1024x8x8xf32>
// CHECK:           return %[[VAL_11:.*]] : tensor<1x1024x8x8xf32>
// CHECK:         }
func.func @unfuse_conv_2d_tensor_add(%ifm : tensor<1x1024x10x10xf32>, %summand : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>
    %init = tensor.splat %zero : tensor<1x1024x8x8xf32>
    %result = linalg.conv_2d_tensor_add
        {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>}
        ins(%ifm, %summand, %weights, %bias : tensor<1x1024x10x10xf32>, tensor<1x1024x8x8xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>)
        outs(%init : tensor<1x1024x8x8xf32>)
        -> tensor<1x1024x8x8xf32>
    return %result : tensor<1x1024x8x8xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_relu(
// CHECK-SAME:                              %[[VAL_0:.*]]: tensor<1x1024x17x17xf32>) -> tensor<1x1024x7x7xf32> {
// CHECK:           %[[VAL_1:.*]] = linalg.fused(%[[VAL_2:.*]] = %[[VAL_0]] : tensor<1x1024x17x17xf32>) {
// CHECK:             %[[VAL_3:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_4:.*]] = linalg.init_tensor [1, 1024, 7, 7] : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_5:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_7:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_5]] : tensor<1024xf32>) outs(%[[VAL_4]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_8:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<2> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_2]], %[[VAL_6]] : tensor<1x1024x17x17xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_7]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.relu_2d_nchw ins(%[[VAL_8]] : tensor<1x1024x7x7xf32>) outs(%[[VAL_8]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             linalg.yield %[[VAL_9]] : tensor<1x1024x7x7xf32>
// CHECK:           } -> tensor<1x1024x7x7xf32>
// CHECK:           return %[[VAL_10:.*]] : tensor<1x1024x7x7xf32>
// CHECK:         }
func.func @unfuse_conv_2d_relu(%ifm : tensor<1x1024x17x17xf32>) -> tensor<1x1024x7x7xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>
    %init = tensor.splat %zero : tensor<1x1024x7x7xf32>
    %result = linalg.conv_2d_relu
        {dilations = dense<2> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>}
        ins(%ifm, %weights, %bias : tensor<1x1024x17x17xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>)
        outs(%init : tensor<1x1024x7x7xf32>)
        -> tensor<1x1024x7x7xf32>
    return %result : tensor<1x1024x7x7xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_tensor_add_relu(
// CHECK-SAME:                                         %[[VAL_0:.*]]: tensor<1x1024x17x17xf32>,
// CHECK-SAME:                                         %[[VAL_1:.*]]: tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32> {
// CHECK:           %[[VAL_2:.*]] = linalg.fused(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x1024x17x17xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : tensor<1x1024x7x7xf32>) {
// CHECK:             %[[VAL_5:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_6:.*]] = linalg.init_tensor [1, 1024, 7, 7] : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_8:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_7]] : tensor<1024xf32>) outs(%[[VAL_6]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<2> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_3]], %[[VAL_8]] : tensor<1x1024x17x17xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_9]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_11:.*]] = arith.addf %[[VAL_10]], %[[VAL_4]] : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_12:.*]] = linalg.relu_2d_nchw ins(%[[VAL_11]] : tensor<1x1024x7x7xf32>) outs(%[[VAL_11]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             linalg.yield %[[VAL_12]] : tensor<1x1024x7x7xf32>
// CHECK:           } -> tensor<1x1024x7x7xf32>
// CHECK:           return %[[VAL_13:.*]] : tensor<1x1024x7x7xf32>
// CHECK:         }
func.func @unfuse_conv_2d_tensor_add_relu(%ifm : tensor<1x1024x17x17xf32>, %summand : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>
    %init = tensor.splat %zero : tensor<1x1024x7x7xf32>
    %result = linalg.conv_2d_tensor_add_relu
        {dilations = dense<2> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>}
        ins(%ifm, %summand, %weights, %bias : tensor<1x1024x17x17xf32>, tensor<1x1024x7x7xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>)
        outs(%init : tensor<1x1024x7x7xf32>)
        -> tensor<1x1024x7x7xf32>
    return %result : tensor<1x1024x7x7xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_lrelu(
// CHECK-SAME:                               %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
// CHECK:           %[[VAL_1:.*]] = linalg.fused(%[[VAL_2:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_3:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_4:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_5:.*]] = linalg.init_tensor [1, 1024, 13, 13] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_8:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_6]] : tensor<1024xf32>) outs(%[[VAL_5]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_2]], %[[VAL_7]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_8]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_9]], %[[VAL_4]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_9]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_10]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           return %[[VAL_11:.*]] : tensor<1x1024x13x13xf32>
// CHECK:         }
func.func @unfuse_conv_2d_lrelu(%ifm : tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>
    %alpha = arith.constant 2.000000e-02 : f32
    %init = tensor.splat %zero : tensor<1x1024x13x13xf32>
    %result = linalg.conv_2d_lrelu
        {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>}
        ins(%ifm, %weights, %bias, %alpha : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>, f32)
        outs(%init : tensor<1x1024x13x13xf32>)
        -> tensor<1x1024x13x13xf32>
    return %result : tensor<1x1024x13x13xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_tensor_add_lrelu(
// CHECK-SAME:                                          %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>,
// CHECK-SAME:                                          %[[VAL_1:.*]]: tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32> {
// CHECK:           %[[VAL_2:.*]] = linalg.fused(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : tensor<1x1024x13x13xf32>) {
// CHECK:             %[[VAL_5:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_6:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_7:.*]] = linalg.init_tensor [1, 1024, 13, 13] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_8:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_9:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_8]] : tensor<1024xf32>) outs(%[[VAL_7]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_11:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_3]], %[[VAL_9]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_10]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_12:.*]] = arith.addf %[[VAL_11]], %[[VAL_4]] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_13:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_12]], %[[VAL_6]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_12]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_13]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           return %[[VAL_14:.*]] : tensor<1x1024x13x13xf32>
// CHECK:         }
func.func @unfuse_conv_2d_tensor_add_lrelu(%ifm : tensor<1x1024x15x15xf32>, %summand : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>
    %alpha = arith.constant 2.000000e-02 : f32
    %init = tensor.splat %zero : tensor<1x1024x13x13xf32>
    %result = linalg.conv_2d_tensor_add_lrelu
        {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>}
        ins(%ifm, %summand, %weights, %bias, %alpha : tensor<1x1024x15x15xf32>, tensor<1x1024x13x13xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>, f32)
        outs(%init : tensor<1x1024x13x13xf32>)
        -> tensor<1x1024x13x13xf32>
    return %result : tensor<1x1024x13x13xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_lrelu_maxpool(
// CHECK-SAME:                                       %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>) -> tensor<1x1024x7x7xf32> {
// CHECK:           %[[VAL_1:.*]] = linalg.fused(%[[VAL_2:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_3:.*]] = linalg.init_tensor [1, 1024, 13, 13] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_4:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_5:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_7:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_8:.*]] = arith.constant 0xFF800000 : f32
// CHECK:             %[[VAL_9:.*]] = arith.constant dense<0.000000e+00> : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.init_tensor [2, 2] : tensor<2x2xf32>
// CHECK:             %[[VAL_11:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_4]] : tensor<1024xf32>) outs(%[[VAL_3]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_12:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_2]], %[[VAL_5]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_11]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_13:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_12]], %[[VAL_7]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_12]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_14:.*]] = tensor.pad %[[VAL_13]] low[0, 0, 0, 0] high[0, 0, 1, 1] {
// CHECK:             ^bb0(%[[VAL_15:.*]]: index, %[[VAL_16:.*]]: index, %[[VAL_17:.*]]: index, %[[VAL_18:.*]]: index):
// CHECK:               tensor.yield %[[VAL_8]] : f32
// CHECK:             } : tensor<1x1024x13x13xf32> to tensor<1x1024x14x14xf32>
// CHECK:             %[[VAL_19:.*]] = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_20:.*]], %[[VAL_10]] : tensor<1x1024x14x14xf32>, tensor<2x2xf32>) outs(%[[VAL_9]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             linalg.yield %[[VAL_19]] : tensor<1x1024x7x7xf32>
// CHECK:           } -> tensor<1x1024x7x7xf32>
// CHECK:           return %[[VAL_21:.*]] : tensor<1x1024x7x7xf32>
// CHECK:         }
func.func @unfuse_conv_2d_lrelu_maxpool(%ifm : tensor<1x1024x15x15xf32>) -> tensor<1x1024x7x7xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>
    %alpha = arith.constant 2.000000e-02 : f32

    %init = tensor.splat %zero : tensor<1x1024x7x7xf32>
    %result = linalg.conv_2d_lrelu_maxpool
        {
            dilations = dense<1> : tensor<2xi64>,
            strides = dense<1> : tensor<2xi64>,
            mp_kernel_size = dense<2> : tensor<2xi64>,
            mp_strides = dense<2> : tensor<2xi64>,
            mp_dilations = dense<1> : tensor<2xi64>,
            mp_padding = dense<[0, 1, 0, 1]> : tensor<4xi64>
        }
        ins(%ifm, %weights, %bias, %alpha : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>, f32)
        outs(%init : tensor<1x1024x7x7xf32>)
        -> tensor<1x1024x7x7xf32>
    return %result : tensor<1x1024x7x7xf32>
}

// -----

// CHECK-LABEL:   func @unfuse_conv_2d_relu_maxpool(
// CHECK-SAME:                                      %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>) -> tensor<1x1024x7x7xf32> {
// CHECK:           %[[VAL_1:.*]] = linalg.fused(%[[VAL_2:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_3:.*]] = linalg.init_tensor [1, 1024, 13, 13] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_4:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_5:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_7:.*]] = arith.constant 0xFF800000 : f32
// CHECK:             %[[VAL_8:.*]] = arith.constant dense<0.000000e+00> : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.init_tensor [2, 2] : tensor<2x2xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_4]] : tensor<1024xf32>) outs(%[[VAL_3]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_11:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_2]], %[[VAL_5]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_10]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_12:.*]] = linalg.relu_2d_nchw ins(%[[VAL_11]] : tensor<1x1024x13x13xf32>) outs(%[[VAL_11]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_13:.*]] = tensor.pad %[[VAL_12]] low[0, 0, 0, 0] high[0, 0, 1, 1] {
// CHECK:             ^bb0(%[[VAL_14:.*]]: index, %[[VAL_15:.*]]: index, %[[VAL_16:.*]]: index, %[[VAL_17:.*]]: index):
// CHECK:               tensor.yield %[[VAL_7]] : f32
// CHECK:             } : tensor<1x1024x13x13xf32> to tensor<1x1024x14x14xf32>
// CHECK:             %[[VAL_18:.*]] = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_19:.*]], %[[VAL_9]] : tensor<1x1024x14x14xf32>, tensor<2x2xf32>) outs(%[[VAL_8]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             linalg.yield %[[VAL_18]] : tensor<1x1024x7x7xf32>
// CHECK:           } -> tensor<1x1024x7x7xf32>
// CHECK:           return %[[VAL_20:.*]] : tensor<1x1024x7x7xf32>
// CHECK:         }
func.func @unfuse_conv_2d_relu_maxpool(%ifm : tensor<1x1024x15x15xf32>) -> tensor<1x1024x7x7xf32> {
    %zero = arith.constant 0.0 : f32
    %weights = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
    %bias = arith.constant dense<3.000000e-01> : tensor<1024xf32>

    %init = tensor.splat %zero : tensor<1x1024x7x7xf32>
    %result = linalg.conv_2d_relu_maxpool
        {
            dilations = dense<1> : tensor<2xi64>,
            strides = dense<1> : tensor<2xi64>,
            mp_kernel_size = dense<2> : tensor<2xi64>,
            mp_strides = dense<2> : tensor<2xi64>,
            mp_dilations = dense<1> : tensor<2xi64>,
            mp_padding = dense<[0, 1, 0, 1]> : tensor<4xi64>
        }
        ins(%ifm, %weights, %bias : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>, tensor<1024xf32>)
        outs(%init : tensor<1x1024x7x7xf32>)
        -> tensor<1x1024x7x7xf32>
    return %result : tensor<1x1024x7x7xf32>
}
