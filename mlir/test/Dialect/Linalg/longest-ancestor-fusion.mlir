// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// The script is designed to make adding checks to
// a test case fast, it is *not* designed to be authoritative
// about what constitutes a good test! The CHECK should be
// minimized and named to reflect the test intent.

// RUN: mlir-opt %s -split-input-file -linalg-structural-fusion -cse | FileCheck %s

// CHECK-LABEL:   func @ancestor_conv_2d_tensor_add(
// CHECK-SAME:                                      %[[VAL_0:.*]]: tensor<1x1024x10x10xf32>,
// CHECK-SAME:                                      %[[VAL_1:.*]]: tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32> {
// CHECK:           %[[VAL_2:.*]] = linalg.subgraph(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x1024x10x10xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : tensor<1x1024x8x8xf32>) {
// CHECK:             %[[VAL_5:.*]] = tensor.empty() : tensor<1x1024x8x8xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_8:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_6]] : tensor<1024xf32>) outs(%[[VAL_5]] : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_3]], %[[VAL_7]] : tensor<1x1024x10x10xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_8]] : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32>
// CHECK:             %[[VAL_10:.*]] = arith.addf %[[VAL_9]], %[[VAL_4]] : tensor<1x1024x8x8xf32>
// CHECK:             linalg.yield %[[VAL_10]] : tensor<1x1024x8x8xf32>
// CHECK:           } -> tensor<1x1024x8x8xf32>
// CHECK:           return %[[VAL_11:.*]] : tensor<1x1024x8x8xf32>
// CHECK:         }
func.func @ancestor_conv_2d_tensor_add(%arg0: tensor<1x1024x10x10xf32>, %arg1: tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32> {
  %cst = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
  %cst_0 = arith.constant dense<3.000000e-01> : tensor<1024xf32>
  %0 = tensor.empty() : tensor<1x1024x8x8xf32>
  %1 = linalg.broadcast_bias_2d_fchw ins(%cst_0 : tensor<1024xf32>) outs(%0 : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32>
  %2 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst : tensor<1x1024x10x10xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x8x8xf32>) -> tensor<1x1024x8x8xf32>
  %3 = arith.addf %2, %arg1 : tensor<1x1024x8x8xf32>
  return %3 : tensor<1x1024x8x8xf32>
}

// -----
// CHECK-LABEL:   func @ancestor_conv_2d_tensor_add_relu(
// CHECK-SAME:                                           %[[VAL_0:.*]]: tensor<1x1024x17x17xf32>,
// CHECK-SAME:                                           %[[VAL_1:.*]]: tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32> {
// CHECK:           %[[VAL_2:.*]] = linalg.subgraph(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x1024x17x17xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : tensor<1x1024x7x7xf32>) {
// CHECK:             %[[VAL_5:.*]] = tensor.empty() : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_8:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_6]] : tensor<1024xf32>) outs(%[[VAL_5]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<2> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_3]], %[[VAL_7]] : tensor<1x1024x17x17xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_8]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_10:.*]] = arith.addf %[[VAL_9]], %[[VAL_4]] : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_11:.*]] = linalg.relu_2d_nchw ins(%[[VAL_10]] : tensor<1x1024x7x7xf32>) outs(%[[VAL_10]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             linalg.yield %[[VAL_11]] : tensor<1x1024x7x7xf32>
// CHECK:           } -> tensor<1x1024x7x7xf32>
// CHECK:           return %[[VAL_12:.*]] : tensor<1x1024x7x7xf32>
// CHECK:         }
func.func @ancestor_conv_2d_tensor_add_relu(%arg0: tensor<1x1024x17x17xf32>, %arg1: tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32> {
  %cst = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
  %cst_0 = arith.constant dense<3.000000e-01> : tensor<1024xf32>
  %0 = tensor.empty() : tensor<1x1024x7x7xf32>
  %1 = linalg.broadcast_bias_2d_fchw ins(%cst_0 : tensor<1024xf32>) outs(%0 : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
  %2 = linalg.conv_2d_nchw_fchw {dilations = dense<2> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%arg0, %cst : tensor<1x1024x17x17xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
  %3 = arith.addf %2, %arg1 : tensor<1x1024x7x7xf32>
  %4 = linalg.relu_2d_nchw ins(%3 : tensor<1x1024x7x7xf32>) outs(%3 : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
  return %4 : tensor<1x1024x7x7xf32>
}

// -----
// CHECK-LABEL:   func @ancestor_conv_2d_tensor_add_lrelu(
// CHECK-SAME:                                            %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>,
// CHECK-SAME:                                            %[[VAL_1:.*]]: tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32> {
// CHECK:           %[[VAL_2:.*]] = linalg.subgraph(%[[VAL_3:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>, %[[VAL_4:.*]] = %[[VAL_1]] : tensor<1x1024x13x13xf32>) {
// CHECK:             %[[VAL_5:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_6:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_8:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_9:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_7]] : tensor<1024xf32>) outs(%[[VAL_6]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_3]], %[[VAL_8]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_9]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_11:.*]] = arith.addf %[[VAL_10]], %[[VAL_4]] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_12:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_11]], %[[VAL_5]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_11]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_12]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           return %[[VAL_13:.*]] : tensor<1x1024x13x13xf32>
// CHECK:         }
func.func @ancestor_conv_2d_tensor_add_lrelu(%arg0: tensor<1x1024x15x15xf32>, %arg1: tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32> {
  %cst = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
  %cst_0 = arith.constant dense<3.000000e-01> : tensor<1024xf32>
  %cst_1 = arith.constant 2.000000e-02 : f32
  %0 = tensor.empty() : tensor<1x1024x13x13xf32>
  %1 = linalg.broadcast_bias_2d_fchw ins(%cst_0 : tensor<1024xf32>) outs(%0 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %2 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %3 = arith.addf %2, %arg1 : tensor<1x1024x13x13xf32>
  %4 = linalg.lrelu_2d_nchw ins(%3, %cst_1 : tensor<1x1024x13x13xf32>, f32) outs(%3 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  return %4 : tensor<1x1024x13x13xf32>
}

// -----
// CHECK-LABEL:   func @ancestor_same_length_conv_2d_tensor_add(
// CHECK-SAME:                                                  %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
// CHECK:           %[[VAL_1:.*]] = linalg.subgraph(%[[VAL_2:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_3:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_4:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_5:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_6:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_4]] : tensor<1024xf32>) outs(%[[VAL_3]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_7:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_2]], %[[VAL_5]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_6]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_7]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           %[[VAL_8:.*]] = linalg.subgraph(%[[VAL_9:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>, %[[VAL_10:.*]] = %[[VAL_11:.*]] : tensor<1x1024x13x13xf32>) {
// CHECK:             %[[VAL_12:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_13:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_14:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_15:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_16:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_14]] : tensor<1024xf32>) outs(%[[VAL_13]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_17:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_9]], %[[VAL_15]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_16]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_18:.*]] = arith.addf %[[VAL_17]], %[[VAL_10]] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_19:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_18]], %[[VAL_12]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_18]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_19]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           return %[[VAL_20:.*]] : tensor<1x1024x13x13xf32>
// CHECK:         }
func.func @ancestor_same_length_conv_2d_tensor_add(%arg0: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
  %cst = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
  %cst_0 = arith.constant dense<3.000000e-01> : tensor<1024xf32>
  %cst_1 = arith.constant 2.000000e-02 : f32
  %0 = tensor.empty() : tensor<1x1024x13x13xf32>
  %1 = linalg.broadcast_bias_2d_fchw ins(%cst_0 : tensor<1024xf32>) outs(%0 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %2 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %3 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %4 = arith.addf %2, %3 : tensor<1x1024x13x13xf32>
  %5 = linalg.lrelu_2d_nchw ins(%4, %cst_1 : tensor<1x1024x13x13xf32>, f32) outs(%4 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>

  return %5 : tensor<1x1024x13x13xf32>
}

// -----
// CHECK-LABEL:   func @ancestor_conv_2d_tensor_add_same(
// CHECK-SAME:                                           %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
// CHECK:           %[[VAL_1:.*]] = linalg.subgraph(%[[VAL_2:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_3:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_4:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_5:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_7:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_5]] : tensor<1024xf32>) outs(%[[VAL_4]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_8:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_2]], %[[VAL_6]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_7]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_9:.*]] = arith.addf %[[VAL_8]], %[[VAL_8]] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_10:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_9]], %[[VAL_3]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_9]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_10]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           return %[[VAL_11:.*]] : tensor<1x1024x13x13xf32>
// CHECK:         }
func.func @ancestor_conv_2d_tensor_add_same(%arg0: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
  %cst = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
  %cst_0 = arith.constant dense<3.000000e-01> : tensor<1024xf32>
  %cst_1 = arith.constant 2.000000e-02 : f32
  %0 = tensor.empty() : tensor<1x1024x13x13xf32>
  %1 = linalg.broadcast_bias_2d_fchw ins(%cst_0 : tensor<1024xf32>) outs(%0 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %2 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %3 = arith.addf %2, %2 : tensor<1x1024x13x13xf32>
  %4 = linalg.lrelu_2d_nchw ins(%3, %cst_1 : tensor<1x1024x13x13xf32>, f32) outs(%3 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  return %4 : tensor<1x1024x13x13xf32>
}

// -----
// NOTE: VAL_1 and VAL_2 are unused after the fusion.
// CHECK-LABEL:   func @ancestor_conv_2d_tensor_add_longer_path(
// CHECK-SAME:                                                  %[[VAL_0:.*]]: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
// CHECK:           %[[VAL_1:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:           %[[VAL_2:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:           %[[VAL_3:.*]] = linalg.subgraph(%[[VAL_4:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_5:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_6:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_7:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_8:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_9:.*]] = arith.constant 0xFF800000 : f32
// CHECK:             %[[VAL_10:.*]] = arith.constant dense<0.000000e+00> : tensor<1x1024x7x7xf32>
// CHECK:             %[[VAL_11:.*]] = tensor.empty() : tensor<2x2xf32>
// CHECK:             %[[VAL_12:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_6]] : tensor<1024xf32>) outs(%[[VAL_5]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_13:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_4]], %[[VAL_7]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_12]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_14:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_13]], %[[VAL_8]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_13]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_15:.*]] = tensor.pad %[[VAL_14]] low[0, 0, 0, 0] high[0, 0, 1, 1] {
// CHECK:             ^bb0(%[[VAL_16:.*]]: index, %[[VAL_17:.*]]: index, %[[VAL_18:.*]]: index, %[[VAL_19:.*]]: index):
// CHECK:               tensor.yield %[[VAL_9]] : f32
// CHECK:             } : tensor<1x1024x13x13xf32> to tensor<1x1024x14x14xf32>
// CHECK:             %[[VAL_20:.*]] = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%[[VAL_21:.*]], %[[VAL_11]] : tensor<1x1024x14x14xf32>, tensor<2x2xf32>) outs(%[[VAL_10]] : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
// CHECK:             linalg.yield %[[VAL_20]] : tensor<1x1024x7x7xf32>
// CHECK:           } -> tensor<1x1024x7x7xf32>
// CHECK:           %[[VAL_22:.*]] = linalg.subgraph(%[[VAL_23:.*]] = %[[VAL_0]] : tensor<1x1024x15x15xf32>) {
// CHECK:             %[[VAL_24:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_25:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_26:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_27:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_25]] : tensor<1024xf32>) outs(%[[VAL_24]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_28:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_23]], %[[VAL_26]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_27]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_28]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           %[[VAL_29:.*]] = linalg.subgraph(%[[VAL_30:.*]] = %[[VAL_31:.*]] : tensor<1x1024x7x7xf32>, %[[VAL_32:.*]] = %[[VAL_33:.*]] : tensor<1x1024x13x13xf32>) {
// CHECK:             %[[VAL_34:.*]] = arith.constant 2.000000e-02 : f32
// CHECK:             %[[VAL_35:.*]] = arith.constant 0.000000e+00 : f32
// CHECK:             %[[VAL_36:.*]] = tensor.pad %[[VAL_30]] low[0, 0, 4, 4] high[0, 0, 4, 4] {
// CHECK:             ^bb0(%[[VAL_37:.*]]: index, %[[VAL_38:.*]]: index, %[[VAL_39:.*]]: index, %[[VAL_40:.*]]: index):
// CHECK:               tensor.yield %[[VAL_35]] : f32
// CHECK:             } : tensor<1x1024x7x7xf32> to tensor<1x1024x15x15xf32>
// CHECK:             %[[VAL_41:.*]] = tensor.empty() : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_42:.*]] = arith.constant dense<3.000000e-01> : tensor<1024xf32>
// CHECK:             %[[VAL_43:.*]] = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
// CHECK:             %[[VAL_44:.*]] = linalg.broadcast_bias_2d_fchw ins(%[[VAL_42]] : tensor<1024xf32>) outs(%[[VAL_41]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_45:.*]] = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%[[VAL_46:.*]], %[[VAL_43]] : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%[[VAL_44]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_47:.*]] = arith.addf %[[VAL_45]], %[[VAL_32]] : tensor<1x1024x13x13xf32>
// CHECK:             %[[VAL_48:.*]] = linalg.lrelu_2d_nchw ins(%[[VAL_47]], %[[VAL_34]] : tensor<1x1024x13x13xf32>, f32) outs(%[[VAL_47]] : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
// CHECK:             linalg.yield %[[VAL_48]] : tensor<1x1024x13x13xf32>
// CHECK:           } -> tensor<1x1024x13x13xf32>
// CHECK:           return %[[VAL_49:.*]] : tensor<1x1024x13x13xf32>
// CHECK:         }
func.func @ancestor_conv_2d_tensor_add_longer_path(%arg0: tensor<1x1024x15x15xf32>) -> tensor<1x1024x13x13xf32> {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : tensor<1x1024x7x7xf32>
  %cst_1 = arith.constant 0xFF800000 : f32
  %cst_2 = arith.constant dense<5.000000e-01> : tensor<1024x1024x3x3xf32>
  %cst_3 = arith.constant dense<3.000000e-01> : tensor<1024xf32>
  %cst_4 = arith.constant 2.000000e-02 : f32
  %0 = tensor.empty() : tensor<1x1024x13x13xf32>
  %1 = linalg.broadcast_bias_2d_fchw ins(%cst_3 : tensor<1024xf32>) outs(%0 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %2 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst_2 : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %3 = linalg.lrelu_2d_nchw ins(%2, %cst_4 : tensor<1x1024x13x13xf32>, f32) outs(%2 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %4 = tensor.pad %3 low[0, 0, 0, 0] high[0, 0, 1, 1] {
  ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):
    tensor.yield %cst_1 : f32
  } : tensor<1x1024x13x13xf32> to tensor<1x1024x14x14xf32>
  %5 = tensor.empty() : tensor<2x2xf32>
  %6 = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins(%4, %5 : tensor<1x1024x14x14xf32>, tensor<2x2xf32>) outs(%cst_0 : tensor<1x1024x7x7xf32>) -> tensor<1x1024x7x7xf32>
  %7 = tensor.pad %6 low[0, 0, 4, 4] high[0, 0, 4, 4] {
    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):
      tensor.yield %cst : f32
    } : tensor<1x1024x7x7xf32> to tensor<1x1024x15x15xf32>
  %8 = linalg.broadcast_bias_2d_fchw ins(%cst_3 : tensor<1024xf32>) outs(%0 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %9 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%7, %cst_2 : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %10 = tensor.empty() : tensor<1x1024x13x13xf32>
  %11 = linalg.broadcast_bias_2d_fchw ins(%cst_3 : tensor<1024xf32>) outs(%0 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %12 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins(%arg0, %cst_2 : tensor<1x1024x15x15xf32>, tensor<1024x1024x3x3xf32>) outs(%1 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>
  %13 = arith.addf %9, %12 : tensor<1x1024x13x13xf32>
  %14 = linalg.lrelu_2d_nchw ins(%13, %cst_4 : tensor<1x1024x13x13xf32>, f32) outs(%13 : tensor<1x1024x13x13xf32>) -> tensor<1x1024x13x13xf32>


  return %14 : tensor<1x1024x13x13xf32>
}
